{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 0 epoch, w is 2.600000, loss if 36.000000\n",
      "After 10 epoch, w is -0.978232, loss if 0.001316\n",
      "After 20 epoch, w is -0.999868, loss if 0.000000\n",
      "After 30 epoch, w is -0.999999, loss if 0.000000\n",
      "After 40 epoch, w is -1.000000, loss if 0.000000\n"
     ]
    }
   ],
   "source": [
    "# 反向传播 backpropagation\n",
    "# 需要先设置为可训练参数\n",
    "w = tf.Variable(tf.constant(5, dtype=tf.float32))\n",
    "lr = 0.2\n",
    "epochs = 40\n",
    "\n",
    "for epoch in range(epochs+1):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = tf.square(w+1)\n",
    "    grads = tape.gradient(loss, w)\n",
    "\n",
    "    w.assign_sub(lr * grads)\n",
    "    if epoch % 10 == 0:\n",
    "        print(\"After %s epoch, w is %f, loss if %f\" % (epoch, w.numpy(), loss)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 张量(tensor)：多维数组(列表)，阶(张量的维度)\n",
    "\n",
    "- 0-D   0阶 标量scalar  s=1 2 3\n",
    "- 1-D   1阶 向量vector  v=[1,2,3]\n",
    "- 2-D   2阶 矩阵matrix  m=[[1,2,3],[4,5,6],[7,8,9]]\n",
    "- n-D   n阶 张量tensor  t=[[[... n个中括号\n",
    "\n",
    "### 数据类型\n",
    "\n",
    "- `tf.int`, `tf.float`...\n",
    "  - tf.int32, tf.float32, tf.float64\n",
    "- `tf.bool`\n",
    "  - tf.const([True, True])\n",
    "- `tf.string`\n",
    "  - tf.constant(\"Hello, world!\")\n",
    "\n",
    "### 如何创建一个 `Tensor`\n",
    "\n",
    "- 创建一个张量 `tf.constant(张量内容, dtype=数据类型(可选))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([1 5], shape=(2,), dtype=int64)\n",
      "<dtype: 'int64'>\n",
      "(2,)\n"
     ]
    }
   ],
   "source": [
    "a = tf.constant([1, 5], dtype=tf.int64)\n",
    "print(a)\n",
    "print(a.dtype)\n",
    "print(a.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 将 `numpy` 的数据类型转换为 `Tensor` 数据类型 `tf.convert_to_tensor(数据名, dtype=数据类型(可选))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4]\n",
      "tf.Tensor([0 1 2 3 4], shape=(5,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "a = np.arange(0, 5)\n",
    "b = tf.convert_to_tensor(a, dtype=tf.int64)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 创建全为0的张量\n",
    "  - `tf.zeros(维度)`\n",
    "- 创建全为1的张量\n",
    "  - `tf.ones(维度)`\n",
    "- 创建全为指定值的张量\n",
    "  - `tf.fill(维度, 指定值)`\n",
    "- 维度：\n",
    "  - 一维 直接写个数\n",
    "  - 二维 用[行 列]\n",
    "  - 多维 用[n,m,j,k......]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(2, 3), dtype=float32)\n",
      "tf.Tensor([1. 1. 1. 1.], shape=(4,), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[9 9]\n",
      " [9 9]], shape=(2, 2), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "a = tf.zeros(((2, 3)))\n",
    "b = tf.ones(4)\n",
    "c = tf.fill((2, 2), 9)\n",
    "print(a)\n",
    "print(b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 生成正态分布的随机数，默认均值为0，标准差为1\n",
    "  - `tf.random.normal(维度, mean=均值, stddev=标准差)`\n",
    "- 生成截断式正太分布的随机数\n",
    "  - `tf.random.truncated_normal(维度, mean=均值, stddev=标准差)`\n",
    "  - 生成的数据取值在 (μ-2δ, μ+2δ)\n",
    "- 生成均匀分布随机数 [minval, maxval)\n",
    "  - `tf.random.uniform(维度, minval=最小值, maxval=最大值)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[-1.2783725  -0.08786917]\n",
      " [-0.31609505 -0.93436253]], shape=(2, 2), dtype=float32)\n",
      "mean(d): tf.Tensor(-0.6541748, shape=(), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[ 0.5581544   0.738902  ]\n",
      " [ 1.6510836  -0.42922652]], shape=(2, 2), dtype=float32)\n",
      "mean(e): tf.Tensor(0.62972844, shape=(), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[0.4863268  0.90099823]\n",
      " [0.13717973 0.51025224]], shape=(2, 2), dtype=float32)\n",
      "mean(f): tf.Tensor(0.5086893, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "d = tf.random.normal((2, 2), mean=0.5, stddev=1)\n",
    "print(d)\n",
    "print(\"mean(d):\", tf.reduce_mean(d))\n",
    "\n",
    "e = tf.random.truncated_normal((2, 2), mean=0.5, stddev=1)\n",
    "print(e)\n",
    "print(\"mean(e):\", tf.reduce_mean(e))\n",
    "\n",
    "f = tf.random.uniform((2, 2), minval=0, maxval=1)\n",
    "print(f)\n",
    "print(\"mean(f):\", tf.reduce_mean(f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `tf.cast` 强制 tensor 转换为该数据类型\n",
    "- `tf.reduce_min` 计算张量维度上元素的最小值\n",
    "- `tf.reduce_max` 计算张量维度上元素的最大值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([1. 2. 3.], shape=(3,), dtype=float64)\n",
      "tf.Tensor([1 2 3], shape=(3,), dtype=int32)\n",
      "tf.Tensor(1, shape=(), dtype=int32) tf.Tensor(3, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "x1 = tf.constant([1., 2., 3.], dtype=tf.float64)\n",
    "print(x1)\n",
    "x2 = tf.cast(x1, tf.int32)\n",
    "print(x2)\n",
    "print(tf.reduce_min(x2), tf.reduce_max(x2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 常用函数\n",
    "\n",
    "#### 理解axis\n",
    "\n",
    "在一个二维张量或数组中，可以通过调整 axis 等于 0或1 控制执行维度。\n",
    "- `axis=0` 代表跨行(经度，down)，而 `axis=1` 代表跨列(纬度，across)\n",
    "- 如果不指定 axis，则所有元素参与计算\n",
    "- 指定哪个维度，哪个维度被压缩\n",
    "\n",
    "![维度的定义](img/img1.png)\n",
    "\n",
    "- `tf.reduce_mean(张量名, axis=操作轴)` 计算张量沿着指定维度的平均值\n",
    "- `tf.reduce_sum(张量名, axis=操作轴)` 计算张量沿着指定维度的和"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1 2 3]\n",
      " [2 2 3]], shape=(2, 3), dtype=int32)\n",
      "tf.Tensor([1 2 3], shape=(3,), dtype=int32)\n",
      "tf.Tensor([6 7], shape=(2,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant([[1, 2, 3], \n",
    "                [2, 2, 3]])\n",
    "print(x)\n",
    "print(tf.reduce_mean(x, axis=0))\n",
    "print(tf.reduce_sum(x, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `tf.Variable()` 将变量**标记为“可训练”**，被标记的变量会在反向传播中记录梯度信息。神经网络训练中，常用该函数标记待训练参数。\n",
    "\n",
    "\n",
    "### Tensorflow中的数学运算\n",
    "\n",
    "- 对应元素的四则运算(只有维度相同的张量才可以进行四则运算)：`tf.add`, `tf.subtract`, `tf.multiply`, `tf.divide`\n",
    "- 开方、次方与平方：`tf.square`, `tf.pow`, `tf.sqrt`\n",
    "- 矩阵乘：`tf.matmul`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[1. 1. 1.]], shape=(1, 3), dtype=float32)\n",
      "tf.Tensor([[3. 3. 3.]], shape=(1, 3), dtype=float32)\n",
      "tf.Tensor([[4. 4. 4.]], shape=(1, 3), dtype=float32)\n",
      "tf.Tensor([[-2. -2. -2.]], shape=(1, 3), dtype=float32)\n",
      "tf.Tensor([[3. 3. 3.]], shape=(1, 3), dtype=float32)\n",
      "tf.Tensor([[3. 3. 3.]], shape=(1, 3), dtype=float32)\n",
      "\n",
      "tf.Tensor([[27. 27. 27.]], shape=(1, 3), dtype=float32)\n",
      "tf.Tensor([[9. 9. 9.]], shape=(1, 3), dtype=float32)\n",
      "tf.Tensor([[1.7320508 1.7320508 1.7320508]], shape=(1, 3), dtype=float32)\n",
      "\n",
      "tf.Tensor(\n",
      "[[6. 6. 6.]\n",
      " [6. 6. 6.]\n",
      " [6. 6. 6.]], shape=(3, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "a = tf.ones([1, 3])\n",
    "b = tf.fill([1, 3], 3.)\n",
    "print(a)\n",
    "print(b)\n",
    "print(tf.add(a, b))\n",
    "print(tf.subtract(a, b))\n",
    "print(tf.multiply(a, b))\n",
    "print(tf.divide(b, a))\n",
    "print()\n",
    "\n",
    "# 平方、n次方、开方\n",
    "print(tf.pow(b, 3))\n",
    "print(tf.square(b))\n",
    "print(tf.sqrt(b))\n",
    "print()\n",
    "\n",
    "# 事项两个矩阵的相乘\n",
    "a = tf.ones([3, 2])\n",
    "b = tf.fill([2, 3], 3.)\n",
    "print(tf.matmul(a, b))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 常用函数 `tf.data.Dataset.from_tensor_slices`\n",
    "\n",
    "- 切分传入张量的第一维度，生成输入特征/标签对，构建数据集\n",
    "- `data = tf.data.Dataset.from_tensor_slices((输入特征, 标签))`\n",
    "- `Numpy` 和 `Tensor` 格式都可用于该语句读入数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<TensorSliceDataset shapes: ((), ()), types: (tf.int32, tf.int32)>\n",
      "(<tf.Tensor: shape=(), dtype=int32, numpy=12>, <tf.Tensor: shape=(), dtype=int32, numpy=0>)\n",
      "(<tf.Tensor: shape=(), dtype=int32, numpy=23>, <tf.Tensor: shape=(), dtype=int32, numpy=1>)\n",
      "(<tf.Tensor: shape=(), dtype=int32, numpy=10>, <tf.Tensor: shape=(), dtype=int32, numpy=1>)\n",
      "(<tf.Tensor: shape=(), dtype=int32, numpy=17>, <tf.Tensor: shape=(), dtype=int32, numpy=0>)\n"
     ]
    }
   ],
   "source": [
    "features = tf.constant([12,23,10,17])\n",
    "labels = tf.constant([0, 1, 1, 0])\n",
    "dataset = tf.data.Dataset.from_tensor_slices((features, labels))\n",
    "print(dataset)\n",
    "for element in dataset:\n",
    "    print(element)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 常用函数 `tf.GradientTape`\n",
    "\n",
    "- `with` 结构记录计算过程，`gradient` 求出张量的梯度\n",
    "\n",
    "```python\n",
    "with tf.GradientTape() as tape:\n",
    "    # 若干计算过程\n",
    "grad = tape.gradient(函数, 对谁求导)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(6.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "with tf.GradientTape() as tape:\n",
    "    w = tf.Variable(tf.constant(3.0))\n",
    "    loss = tf.pow(w, 2)\n",
    "grad = tape.gradient(loss, w)\n",
    "print(grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 常用函数 `enumerate`\n",
    "\n",
    "- `enumerate` 是 `Python` 的内建函数，它可遍历每个元素(如列表、元组或字符串)，**组合为：索引、元素**，常在 `for` 循环中使用。\n",
    "- `enumerate()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 one\n",
      "1 two\n",
      "2 three\n"
     ]
    }
   ],
   "source": [
    "seq = ['one', 'two', 'three']\n",
    "for i, element in enumerate(seq):\n",
    "    print(i, element)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 常用函数 `tf.one_hot`\n",
    "\n",
    "- 独热编码(one-hot encoding)：在分类问题中，常用独热编码做标签，标记类别：1表示是，0表示非。\n",
    "- `tf.one_hot()` 函数将待转换数据，转换为 `one-hot` 形式的数据输出。\n",
    "- `tf.one_hot(待转换数据, depth=几分类)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]], shape=(3, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "classes = 3\n",
    "labels = tf.constant([1,0,2])   # 输入的元素值最小为0，最大为2\n",
    "output = tf.one_hot(labels, depth=classes)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 常用函数 `tf.nn.softmax`\n",
    "\n",
    "- 使输出符合概率分布，进而与独热码形式的标签作比较\n",
    "- 当n分类的n个输出(y0,y1,...,yn-1)通过 `softmax()` 函数，便符合概率分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After softmax, y_pro is: tf.Tensor([0.25598174 0.69583046 0.04818781], shape=(3,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "y = tf.constant([1.01, 2.01, -0.66])\n",
    "y_pro = tf.nn.softmax(y)\n",
    "print(\"After softmax, y_pro is:\", y_pro)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 常用函数 `assign_sub`\n",
    "\n",
    "- 赋值操作，更新参数的值并返回\n",
    "- 调用 `assign_sub` 前，先用 `tf.Variable` 定义变量 w 为可训练(可自更新)。\n",
    "- `w.assign_sub(w要自减的内容)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=() dtype=int32, numpy=3>\n"
     ]
    }
   ],
   "source": [
    "w = tf.Variable(4)\n",
    "w.assign_sub(1)\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 常用函数 `tf.argmax`\n",
    "\n",
    "- 返回张量沿指定维度最大值的索引\n",
    "- `tf.argmax(张量名, axis=操作轴)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]\n",
      " [2 3 4]\n",
      " [5 4 3]\n",
      " [8 7 2]]\n",
      "tf.Tensor([3 3 1], shape=(3,), dtype=int64)\n",
      "tf.Tensor([2 2 0 0], shape=(4,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "test = np.array([[1, 2, 3], [2, 3, 4], [5, 4, 3], [8, 7, 2]])\n",
    "print(test)\n",
    "print(tf.argmax(test, axis=0))\n",
    "print(tf.argmax(test, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 鸢尾花数据集(Iris)\n",
    "\n",
    "- 数据集介绍\n",
    "\n",
    "共有数据150组，每组包括花萼长、花萼宽、花瓣长、花瓣宽4个输入特征。同时给出了，这一组特征对应的鸢尾花类别。类别包括狗尾鸢尾、杂色鸢尾，弗吉尼亚鸢尾三类，分类用数字0，1，2表示。\n",
    "\n",
    "- 从 `sklearn` 包 `datasets` 读入数据集，语法为：\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import load_iris\n",
    "x_data = load_iris().data   # 返回iris数据集所有输入特征\n",
    "y_data = load_iris().target # 返回iris数据集所有标签\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_data from datasets:\n",
      " [[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]\n",
      " [5.4 3.9 1.7 0.4]\n",
      " [4.6 3.4 1.4 0.3]\n",
      " [5.  3.4 1.5 0.2]\n",
      " [4.4 2.9 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [5.4 3.7 1.5 0.2]\n",
      " [4.8 3.4 1.6 0.2]\n",
      " [4.8 3.  1.4 0.1]\n",
      " [4.3 3.  1.1 0.1]\n",
      " [5.8 4.  1.2 0.2]\n",
      " [5.7 4.4 1.5 0.4]\n",
      " [5.4 3.9 1.3 0.4]\n",
      " [5.1 3.5 1.4 0.3]\n",
      " [5.7 3.8 1.7 0.3]\n",
      " [5.1 3.8 1.5 0.3]\n",
      " [5.4 3.4 1.7 0.2]\n",
      " [5.1 3.7 1.5 0.4]\n",
      " [4.6 3.6 1.  0.2]\n",
      " [5.1 3.3 1.7 0.5]\n",
      " [4.8 3.4 1.9 0.2]\n",
      " [5.  3.  1.6 0.2]\n",
      " [5.  3.4 1.6 0.4]\n",
      " [5.2 3.5 1.5 0.2]\n",
      " [5.2 3.4 1.4 0.2]\n",
      " [4.7 3.2 1.6 0.2]\n",
      " [4.8 3.1 1.6 0.2]\n",
      " [5.4 3.4 1.5 0.4]\n",
      " [5.2 4.1 1.5 0.1]\n",
      " [5.5 4.2 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.2]\n",
      " [5.  3.2 1.2 0.2]\n",
      " [5.5 3.5 1.3 0.2]\n",
      " [4.9 3.6 1.4 0.1]\n",
      " [4.4 3.  1.3 0.2]\n",
      " [5.1 3.4 1.5 0.2]\n",
      " [5.  3.5 1.3 0.3]\n",
      " [4.5 2.3 1.3 0.3]\n",
      " [4.4 3.2 1.3 0.2]\n",
      " [5.  3.5 1.6 0.6]\n",
      " [5.1 3.8 1.9 0.4]\n",
      " [4.8 3.  1.4 0.3]\n",
      " [5.1 3.8 1.6 0.2]\n",
      " [4.6 3.2 1.4 0.2]\n",
      " [5.3 3.7 1.5 0.2]\n",
      " [5.  3.3 1.4 0.2]\n",
      " [7.  3.2 4.7 1.4]\n",
      " [6.4 3.2 4.5 1.5]\n",
      " [6.9 3.1 4.9 1.5]\n",
      " [5.5 2.3 4.  1.3]\n",
      " [6.5 2.8 4.6 1.5]\n",
      " [5.7 2.8 4.5 1.3]\n",
      " [6.3 3.3 4.7 1.6]\n",
      " [4.9 2.4 3.3 1. ]\n",
      " [6.6 2.9 4.6 1.3]\n",
      " [5.2 2.7 3.9 1.4]\n",
      " [5.  2.  3.5 1. ]\n",
      " [5.9 3.  4.2 1.5]\n",
      " [6.  2.2 4.  1. ]\n",
      " [6.1 2.9 4.7 1.4]\n",
      " [5.6 2.9 3.6 1.3]\n",
      " [6.7 3.1 4.4 1.4]\n",
      " [5.6 3.  4.5 1.5]\n",
      " [5.8 2.7 4.1 1. ]\n",
      " [6.2 2.2 4.5 1.5]\n",
      " [5.6 2.5 3.9 1.1]\n",
      " [5.9 3.2 4.8 1.8]\n",
      " [6.1 2.8 4.  1.3]\n",
      " [6.3 2.5 4.9 1.5]\n",
      " [6.1 2.8 4.7 1.2]\n",
      " [6.4 2.9 4.3 1.3]\n",
      " [6.6 3.  4.4 1.4]\n",
      " [6.8 2.8 4.8 1.4]\n",
      " [6.7 3.  5.  1.7]\n",
      " [6.  2.9 4.5 1.5]\n",
      " [5.7 2.6 3.5 1. ]\n",
      " [5.5 2.4 3.8 1.1]\n",
      " [5.5 2.4 3.7 1. ]\n",
      " [5.8 2.7 3.9 1.2]\n",
      " [6.  2.7 5.1 1.6]\n",
      " [5.4 3.  4.5 1.5]\n",
      " [6.  3.4 4.5 1.6]\n",
      " [6.7 3.1 4.7 1.5]\n",
      " [6.3 2.3 4.4 1.3]\n",
      " [5.6 3.  4.1 1.3]\n",
      " [5.5 2.5 4.  1.3]\n",
      " [5.5 2.6 4.4 1.2]\n",
      " [6.1 3.  4.6 1.4]\n",
      " [5.8 2.6 4.  1.2]\n",
      " [5.  2.3 3.3 1. ]\n",
      " [5.6 2.7 4.2 1.3]\n",
      " [5.7 3.  4.2 1.2]\n",
      " [5.7 2.9 4.2 1.3]\n",
      " [6.2 2.9 4.3 1.3]\n",
      " [5.1 2.5 3.  1.1]\n",
      " [5.7 2.8 4.1 1.3]\n",
      " [6.3 3.3 6.  2.5]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [7.1 3.  5.9 2.1]\n",
      " [6.3 2.9 5.6 1.8]\n",
      " [6.5 3.  5.8 2.2]\n",
      " [7.6 3.  6.6 2.1]\n",
      " [4.9 2.5 4.5 1.7]\n",
      " [7.3 2.9 6.3 1.8]\n",
      " [6.7 2.5 5.8 1.8]\n",
      " [7.2 3.6 6.1 2.5]\n",
      " [6.5 3.2 5.1 2. ]\n",
      " [6.4 2.7 5.3 1.9]\n",
      " [6.8 3.  5.5 2.1]\n",
      " [5.7 2.5 5.  2. ]\n",
      " [5.8 2.8 5.1 2.4]\n",
      " [6.4 3.2 5.3 2.3]\n",
      " [6.5 3.  5.5 1.8]\n",
      " [7.7 3.8 6.7 2.2]\n",
      " [7.7 2.6 6.9 2.3]\n",
      " [6.  2.2 5.  1.5]\n",
      " [6.9 3.2 5.7 2.3]\n",
      " [5.6 2.8 4.9 2. ]\n",
      " [7.7 2.8 6.7 2. ]\n",
      " [6.3 2.7 4.9 1.8]\n",
      " [6.7 3.3 5.7 2.1]\n",
      " [7.2 3.2 6.  1.8]\n",
      " [6.2 2.8 4.8 1.8]\n",
      " [6.1 3.  4.9 1.8]\n",
      " [6.4 2.8 5.6 2.1]\n",
      " [7.2 3.  5.8 1.6]\n",
      " [7.4 2.8 6.1 1.9]\n",
      " [7.9 3.8 6.4 2. ]\n",
      " [6.4 2.8 5.6 2.2]\n",
      " [6.3 2.8 5.1 1.5]\n",
      " [6.1 2.6 5.6 1.4]\n",
      " [7.7 3.  6.1 2.3]\n",
      " [6.3 3.4 5.6 2.4]\n",
      " [6.4 3.1 5.5 1.8]\n",
      " [6.  3.  4.8 1.8]\n",
      " [6.9 3.1 5.4 2.1]\n",
      " [6.7 3.1 5.6 2.4]\n",
      " [6.9 3.1 5.1 2.3]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [6.8 3.2 5.9 2.3]\n",
      " [6.7 3.3 5.7 2.5]\n",
      " [6.7 3.  5.2 2.3]\n",
      " [6.3 2.5 5.  1.9]\n",
      " [6.5 3.  5.2 2. ]\n",
      " [6.2 3.4 5.4 2.3]\n",
      " [5.9 3.  5.1 1.8]]\n",
      "y_data from datasets:\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n",
      "X_data add a column:\n",
      "      花萼长度  花萼宽度  花瓣长度  花瓣宽度  类别\n",
      "0         5.1       3.5       1.4       0.2     0\n",
      "1         4.9       3.0       1.4       0.2     0\n",
      "2         4.7       3.2       1.3       0.2     0\n",
      "3         4.6       3.1       1.5       0.2     0\n",
      "4         5.0       3.6       1.4       0.2     0\n",
      "..        ...       ...       ...       ...   ...\n",
      "145       6.7       3.0       5.2       2.3     2\n",
      "146       6.3       2.5       5.0       1.9     2\n",
      "147       6.5       3.0       5.2       2.0     2\n",
      "148       6.2       3.4       5.4       2.3     2\n",
      "149       5.9       3.0       5.1       1.8     2\n",
      "\n",
      "[150 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "import pandas as pd\n",
    "\n",
    "x_data = load_iris().data\n",
    "y_data = load_iris().target\n",
    "print(\"x_data from datasets:\\n\", x_data)\n",
    "print(\"y_data from datasets:\\n\", y_data)\n",
    "\n",
    "x_data = pd.DataFrame(x_data, columns=['花萼长度', '花萼宽度', '花瓣长度', '花瓣宽度'])\n",
    "pd.set_option(\"display.unicode.east_asian_width\", True)\n",
    "\n",
    "x_data['类别'] = y_data     # 新增一列，列标签为“类别”，数据为y_data\n",
    "print(\"X_data add a column:\\n\", x_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 神经网络实现鸢尾花分类\n",
    "\n",
    "- 准备数据\n",
    "  - 数据集读入\n",
    "  - 数据集乱序\n",
    "  - 生成训练集和测试集(即 x_train/y_train, x_test/y_test)\n",
    "  - 配成(输入特征, 标签)对，每次读入一小撮(batch)\n",
    "- 搭建网络\n",
    "  - 定义神经网络中所有可训练参数\n",
    "- 参数优化\n",
    "  - 嵌套循环迭代，`with` 结构更新参数，显示当前 `loss`\n",
    "- 测试效果\n",
    "  - 计算当前参数前向传播后的准确率，显示当前 `acc`\n",
    "- `acc/loss` 可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. loss: 0.18948042020201683\n",
      "Test_acc 0.7\n",
      "-----------------------\n",
      "Epoch 1. loss: 0.17079350352287292\n",
      "Test_acc 0.7\n",
      "-----------------------\n",
      "Epoch 2. loss: 0.16424234583973885\n",
      "Test_acc 0.7\n",
      "-----------------------\n",
      "Epoch 3. loss: 0.15846453234553337\n",
      "Test_acc 0.7\n",
      "-----------------------\n",
      "Epoch 4. loss: 0.1534098945558071\n",
      "Test_acc 0.7\n",
      "-----------------------\n",
      "Epoch 5. loss: 0.14898332580924034\n",
      "Test_acc 0.7\n",
      "-----------------------\n",
      "Epoch 6. loss: 0.145090252161026\n",
      "Test_acc 0.7\n",
      "-----------------------\n",
      "Epoch 7. loss: 0.14164571277797222\n",
      "Test_acc 0.7\n",
      "-----------------------\n",
      "Epoch 8. loss: 0.13857670687139034\n",
      "Test_acc 0.7\n",
      "-----------------------\n",
      "Epoch 9. loss: 0.13582199066877365\n",
      "Test_acc 0.7\n",
      "-----------------------\n",
      "Epoch 10. loss: 0.13333084248006344\n",
      "Test_acc 0.7\n",
      "-----------------------\n",
      "Epoch 11. loss: 0.13106147572398186\n",
      "Test_acc 0.7\n",
      "-----------------------\n",
      "Epoch 12. loss: 0.1289796158671379\n",
      "Test_acc 0.7\n",
      "-----------------------\n",
      "Epoch 13. loss: 0.12705708667635918\n",
      "Test_acc 0.7\n",
      "-----------------------\n",
      "Epoch 14. loss: 0.12527069635689259\n",
      "Test_acc 0.7\n",
      "-----------------------\n",
      "Epoch 15. loss: 0.123601283878088\n",
      "Test_acc 0.7\n",
      "-----------------------\n",
      "Epoch 16. loss: 0.12203298509120941\n",
      "Test_acc 0.7\n",
      "-----------------------\n",
      "Epoch 17. loss: 0.12055259011685848\n",
      "Test_acc 0.7\n",
      "-----------------------\n",
      "Epoch 18. loss: 0.11914904043078423\n",
      "Test_acc 0.7\n",
      "-----------------------\n",
      "Epoch 19. loss: 0.11781305819749832\n",
      "Test_acc 0.7\n",
      "-----------------------\n",
      "Epoch 20. loss: 0.11653680726885796\n",
      "Test_acc 0.7\n",
      "-----------------------\n",
      "Epoch 21. loss: 0.11531366594135761\n",
      "Test_acc 0.7\n",
      "-----------------------\n",
      "Epoch 22. loss: 0.11413798481225967\n",
      "Test_acc 0.7\n",
      "-----------------------\n",
      "Epoch 23. loss: 0.11300491727888584\n",
      "Test_acc 0.7\n",
      "-----------------------\n",
      "Epoch 24. loss: 0.11191033944487572\n",
      "Test_acc 0.7\n",
      "-----------------------\n",
      "Epoch 25. loss: 0.11085066385567188\n",
      "Test_acc 0.7\n",
      "-----------------------\n",
      "Epoch 26. loss: 0.10982280783355236\n",
      "Test_acc 0.7\n",
      "-----------------------\n",
      "Epoch 27. loss: 0.10882409289479256\n",
      "Test_acc 0.7\n",
      "-----------------------\n",
      "Epoch 28. loss: 0.10785217210650444\n",
      "Test_acc 0.7\n",
      "-----------------------\n",
      "Epoch 29. loss: 0.10690500028431416\n",
      "Test_acc 0.7\n",
      "-----------------------\n",
      "Epoch 30. loss: 0.10598078556358814\n",
      "Test_acc 0.7333333333333333\n",
      "-----------------------\n",
      "Epoch 31. loss: 0.1050779428333044\n",
      "Test_acc 0.7333333333333333\n",
      "-----------------------\n",
      "Epoch 32. loss: 0.10419508814811707\n",
      "Test_acc 0.7333333333333333\n",
      "-----------------------\n",
      "Epoch 33. loss: 0.10333097353577614\n",
      "Test_acc 0.7333333333333333\n",
      "-----------------------\n",
      "Epoch 34. loss: 0.10248450189828873\n",
      "Test_acc 0.7333333333333333\n",
      "-----------------------\n",
      "Epoch 35. loss: 0.10165469907224178\n",
      "Test_acc 0.7333333333333333\n",
      "-----------------------\n",
      "Epoch 36. loss: 0.10084068402647972\n",
      "Test_acc 0.7333333333333333\n",
      "-----------------------\n",
      "Epoch 37. loss: 0.10004167072474957\n",
      "Test_acc 0.7333333333333333\n",
      "-----------------------\n",
      "Epoch 38. loss: 0.09925694763660431\n",
      "Test_acc 0.7333333333333333\n",
      "-----------------------\n",
      "Epoch 39. loss: 0.09848589077591896\n",
      "Test_acc 0.7333333333333333\n",
      "-----------------------\n",
      "Epoch 40. loss: 0.09772790595889091\n",
      "Test_acc 0.7333333333333333\n",
      "-----------------------\n",
      "Epoch 41. loss: 0.09698247350752354\n",
      "Test_acc 0.7333333333333333\n",
      "-----------------------\n",
      "Epoch 42. loss: 0.09624912962317467\n",
      "Test_acc 0.7333333333333333\n",
      "-----------------------\n",
      "Epoch 43. loss: 0.09552742727100849\n",
      "Test_acc 0.7333333333333333\n",
      "-----------------------\n",
      "Epoch 44. loss: 0.09481697157025337\n",
      "Test_acc 0.7666666666666667\n",
      "-----------------------\n",
      "Epoch 45. loss: 0.09411739744246006\n",
      "Test_acc 0.7666666666666667\n",
      "-----------------------\n",
      "Epoch 46. loss: 0.09342836029827595\n",
      "Test_acc 0.8333333333333334\n",
      "-----------------------\n",
      "Epoch 47. loss: 0.09274955838918686\n",
      "Test_acc 0.8333333333333334\n",
      "-----------------------\n",
      "Epoch 48. loss: 0.09208070300519466\n",
      "Test_acc 0.8333333333333334\n",
      "-----------------------\n",
      "Epoch 49. loss: 0.09142152033746243\n",
      "Test_acc 0.8333333333333334\n",
      "-----------------------\n",
      "Epoch 50. loss: 0.09077177196741104\n",
      "Test_acc 0.8333333333333334\n",
      "-----------------------\n",
      "Epoch 51. loss: 0.09013119898736477\n",
      "Test_acc 0.8333333333333334\n",
      "-----------------------\n",
      "Epoch 52. loss: 0.08949962630867958\n",
      "Test_acc 0.8333333333333334\n",
      "-----------------------\n",
      "Epoch 53. loss: 0.08887680992484093\n",
      "Test_acc 0.8333333333333334\n",
      "-----------------------\n",
      "Epoch 54. loss: 0.08826256357133389\n",
      "Test_acc 0.8666666666666667\n",
      "-----------------------\n",
      "Epoch 55. loss: 0.08765671215951443\n",
      "Test_acc 0.8666666666666667\n",
      "-----------------------\n",
      "Epoch 56. loss: 0.08705908618867397\n",
      "Test_acc 0.8666666666666667\n",
      "-----------------------\n",
      "Epoch 57. loss: 0.0864695142954588\n",
      "Test_acc 0.8666666666666667\n",
      "-----------------------\n",
      "Epoch 58. loss: 0.08588782697916031\n",
      "Test_acc 0.8666666666666667\n",
      "-----------------------\n",
      "Epoch 59. loss: 0.08531389199197292\n",
      "Test_acc 0.8666666666666667\n",
      "-----------------------\n",
      "Epoch 60. loss: 0.084747564047575\n",
      "Test_acc 0.8666666666666667\n",
      "-----------------------\n",
      "Epoch 61. loss: 0.08418869227170944\n",
      "Test_acc 0.8666666666666667\n",
      "-----------------------\n",
      "Epoch 62. loss: 0.08363715745508671\n",
      "Test_acc 0.8666666666666667\n",
      "-----------------------\n",
      "Epoch 63. loss: 0.08309282548725605\n",
      "Test_acc 0.8666666666666667\n",
      "-----------------------\n",
      "Epoch 64. loss: 0.08255556970834732\n",
      "Test_acc 0.8666666666666667\n",
      "-----------------------\n",
      "Epoch 65. loss: 0.08202527649700642\n",
      "Test_acc 0.8666666666666667\n",
      "-----------------------\n",
      "Epoch 66. loss: 0.08150182478129864\n",
      "Test_acc 0.8666666666666667\n",
      "-----------------------\n",
      "Epoch 67. loss: 0.08098511770367622\n",
      "Test_acc 0.8666666666666667\n",
      "-----------------------\n",
      "Epoch 68. loss: 0.08047502301633358\n",
      "Test_acc 0.8666666666666667\n",
      "-----------------------\n",
      "Epoch 69. loss: 0.07997145131230354\n",
      "Test_acc 0.8666666666666667\n",
      "-----------------------\n",
      "Epoch 70. loss: 0.07947429269552231\n",
      "Test_acc 0.8666666666666667\n",
      "-----------------------\n",
      "Epoch 71. loss: 0.07898344472050667\n",
      "Test_acc 0.8666666666666667\n",
      "-----------------------\n",
      "Epoch 72. loss: 0.07849881984293461\n",
      "Test_acc 0.8666666666666667\n",
      "-----------------------\n",
      "Epoch 73. loss: 0.07802030630409718\n",
      "Test_acc 0.8666666666666667\n",
      "-----------------------\n",
      "Epoch 74. loss: 0.07754782028496265\n",
      "Test_acc 0.8666666666666667\n",
      "-----------------------\n",
      "Epoch 75. loss: 0.07708126865327358\n",
      "Test_acc 0.8666666666666667\n",
      "-----------------------\n",
      "Epoch 76. loss: 0.0766205508261919\n",
      "Test_acc 0.8666666666666667\n",
      "-----------------------\n",
      "Epoch 77. loss: 0.07616558857262135\n",
      "Test_acc 0.8666666666666667\n",
      "-----------------------\n",
      "Epoch 78. loss: 0.07571630366146564\n",
      "Test_acc 0.8666666666666667\n",
      "-----------------------\n",
      "Epoch 79. loss: 0.07527258433401585\n",
      "Test_acc 0.8666666666666667\n",
      "-----------------------\n",
      "Epoch 80. loss: 0.07483436539769173\n",
      "Test_acc 0.8666666666666667\n",
      "-----------------------\n",
      "Epoch 81. loss: 0.07440156489610672\n",
      "Test_acc 0.8666666666666667\n",
      "-----------------------\n",
      "Epoch 82. loss: 0.07397409342229366\n",
      "Test_acc 0.8666666666666667\n",
      "-----------------------\n",
      "Epoch 83. loss: 0.07355188019573689\n",
      "Test_acc 0.8666666666666667\n",
      "-----------------------\n",
      "Epoch 84. loss: 0.07313484326004982\n",
      "Test_acc 0.8666666666666667\n",
      "-----------------------\n",
      "Epoch 85. loss: 0.07272289879620075\n",
      "Test_acc 0.8666666666666667\n",
      "-----------------------\n",
      "Epoch 86. loss: 0.07231597974896431\n",
      "Test_acc 0.8666666666666667\n",
      "-----------------------\n",
      "Epoch 87. loss: 0.07191401161253452\n",
      "Test_acc 0.9\n",
      "-----------------------\n",
      "Epoch 88. loss: 0.07151691429316998\n",
      "Test_acc 0.9\n",
      "-----------------------\n",
      "Epoch 89. loss: 0.07112461142241955\n",
      "Test_acc 0.9\n",
      "-----------------------\n",
      "Epoch 90. loss: 0.07073705084621906\n",
      "Test_acc 0.9\n",
      "-----------------------\n",
      "Epoch 91. loss: 0.0703541487455368\n",
      "Test_acc 0.9\n",
      "-----------------------\n",
      "Epoch 92. loss: 0.0699758306145668\n",
      "Test_acc 0.9\n",
      "-----------------------\n",
      "Epoch 93. loss: 0.06960204988718033\n",
      "Test_acc 0.9\n",
      "-----------------------\n",
      "Epoch 94. loss: 0.06923272088170052\n",
      "Test_acc 0.9\n",
      "-----------------------\n",
      "Epoch 95. loss: 0.06886777840554714\n",
      "Test_acc 0.9\n",
      "-----------------------\n",
      "Epoch 96. loss: 0.06850716099143028\n",
      "Test_acc 0.9\n",
      "-----------------------\n",
      "Epoch 97. loss: 0.06815080251544714\n",
      "Test_acc 0.9\n",
      "-----------------------\n",
      "Epoch 98. loss: 0.06779865175485611\n",
      "Test_acc 0.9\n",
      "-----------------------\n",
      "Epoch 99. loss: 0.06745063979178667\n",
      "Test_acc 0.9\n",
      "-----------------------\n",
      "Epoch 100. loss: 0.06710668560117483\n",
      "Test_acc 0.9\n",
      "-----------------------\n",
      "Epoch 101. loss: 0.06676676496863365\n",
      "Test_acc 0.9\n",
      "-----------------------\n",
      "Epoch 102. loss: 0.06643079686909914\n",
      "Test_acc 0.9\n",
      "-----------------------\n",
      "Epoch 103. loss: 0.0660987263545394\n",
      "Test_acc 0.9\n",
      "-----------------------\n",
      "Epoch 104. loss: 0.06577049009501934\n",
      "Test_acc 0.9\n",
      "-----------------------\n",
      "Epoch 105. loss: 0.06544603873044252\n",
      "Test_acc 0.9\n",
      "-----------------------\n",
      "Epoch 106. loss: 0.06512531638145447\n",
      "Test_acc 0.9\n",
      "-----------------------\n",
      "Epoch 107. loss: 0.06480826996266842\n",
      "Test_acc 0.9\n",
      "-----------------------\n",
      "Epoch 108. loss: 0.0644948398694396\n",
      "Test_acc 0.9\n",
      "-----------------------\n",
      "Epoch 109. loss: 0.06418497581034899\n",
      "Test_acc 0.9\n",
      "-----------------------\n",
      "Epoch 110. loss: 0.06387862097471952\n",
      "Test_acc 0.9\n",
      "-----------------------\n",
      "Epoch 111. loss: 0.06357573252171278\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 112. loss: 0.06327624805271626\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 113. loss: 0.06298013124614954\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 114. loss: 0.06268732249736786\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 115. loss: 0.062397767789661884\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 116. loss: 0.062111442908644676\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 117. loss: 0.06182827893644571\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 118. loss: 0.061548239551484585\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 119. loss: 0.06127128563821316\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 120. loss: 0.06099734175950289\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 121. loss: 0.06072640046477318\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 122. loss: 0.0604584040120244\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 123. loss: 0.06019330583512783\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 124. loss: 0.0599310677498579\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 125. loss: 0.059671652503311634\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 126. loss: 0.059415011666715145\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 127. loss: 0.05916110519319773\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 128. loss: 0.05890990328043699\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 129. loss: 0.05866135470569134\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 130. loss: 0.058415429666638374\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 131. loss: 0.05817209556698799\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 132. loss: 0.05793130770325661\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 133. loss: 0.05769303347915411\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 134. loss: 0.05745722260326147\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 135. loss: 0.05722385924309492\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 136. loss: 0.05699290707707405\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 137. loss: 0.056764326989650726\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 138. loss: 0.05653808452188969\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 139. loss: 0.05631414148956537\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 140. loss: 0.05609247460961342\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 141. loss: 0.05587305407971144\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 142. loss: 0.05565584730356932\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 143. loss: 0.055440811440348625\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 144. loss: 0.05522793531417847\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 145. loss: 0.055017161183059216\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 146. loss: 0.05480849463492632\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 147. loss: 0.054601878859102726\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 148. loss: 0.05439731292426586\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 149. loss: 0.05419474188238382\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 150. loss: 0.05399414896965027\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 151. loss: 0.05379550997167826\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 152. loss: 0.05359879229217768\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 153. loss: 0.053403982892632484\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 154. loss: 0.05321103427559137\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 155. loss: 0.0530199334025383\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 156. loss: 0.05283065885305405\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 157. loss: 0.05264319200068712\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 158. loss: 0.05245749466121197\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 159. loss: 0.05227354262024164\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 160. loss: 0.05209132097661495\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 161. loss: 0.05191079806536436\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 162. loss: 0.0517319617792964\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 163. loss: 0.05155479162931442\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 164. loss: 0.051379249431192875\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 165. loss: 0.05120533052831888\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 166. loss: 0.05103300232440233\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 167. loss: 0.05086224991828203\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 168. loss: 0.05069305654615164\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 169. loss: 0.050525388680398464\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 170. loss: 0.05035923980176449\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 171. loss: 0.050194586627185345\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 172. loss: 0.05003140680491924\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 173. loss: 0.049869684502482414\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 174. loss: 0.04970940761268139\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 175. loss: 0.04955054074525833\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 176. loss: 0.04939307738095522\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 177. loss: 0.04923700075596571\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 178. loss: 0.049082293175160885\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 179. loss: 0.04892892204225063\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 180. loss: 0.048776895739138126\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 181. loss: 0.04862618073821068\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 182. loss: 0.048476764000952244\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 183. loss: 0.04832863062620163\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 184. loss: 0.048181770369410515\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 185. loss: 0.04803616181015968\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 186. loss: 0.047891782596707344\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 187. loss: 0.047748628072440624\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 188. loss: 0.04760668147355318\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 189. loss: 0.04746593441814184\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 190. loss: 0.04732635151594877\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 191. loss: 0.047187935560941696\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 192. loss: 0.04705067165195942\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 193. loss: 0.04691453464329243\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 194. loss: 0.046779525466263294\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 195. loss: 0.046645624563097954\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 196. loss: 0.04651281051337719\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 197. loss: 0.04638108517974615\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 198. loss: 0.04625043272972107\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 199. loss: 0.046120830811560154\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 200. loss: 0.04599226824939251\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 201. loss: 0.045864746905863285\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 202. loss: 0.045738247223198414\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 203. loss: 0.045612744987010956\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 204. loss: 0.04548824392259121\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 205. loss: 0.045364719815552235\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 206. loss: 0.04524218197911978\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 207. loss: 0.045120595023036\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 208. loss: 0.04499996546655893\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 209. loss: 0.04488027188926935\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 210. loss: 0.04476151056587696\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 211. loss: 0.04464366193860769\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 212. loss: 0.04452672880142927\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 213. loss: 0.044410690665245056\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 214. loss: 0.0442955382168293\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 215. loss: 0.044181269593536854\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 216. loss: 0.04406786523759365\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 217. loss: 0.04395532049238682\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 218. loss: 0.043843625113368034\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 219. loss: 0.043732766062021255\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 220. loss: 0.04362274240702391\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 221. loss: 0.04351352620869875\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 222. loss: 0.04340513423085213\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 223. loss: 0.043297539465129375\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 224. loss: 0.04319074656814337\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 225. loss: 0.04308473598212004\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 226. loss: 0.04297950305044651\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 227. loss: 0.04287503380328417\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 228. loss: 0.042771329171955585\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 229. loss: 0.04266838077455759\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 230. loss: 0.04256616998463869\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 231. loss: 0.04246469587087631\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 232. loss: 0.04236395284533501\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 233. loss: 0.042263930663466454\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 234. loss: 0.04216462001204491\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 235. loss: 0.04206601623445749\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 236. loss: 0.041968110017478466\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 237. loss: 0.04187089577317238\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 238. loss: 0.04177436139434576\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 239. loss: 0.04167851060628891\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 240. loss: 0.041583322919905186\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 241. loss: 0.04148880019783974\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 242. loss: 0.04139493312686682\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 243. loss: 0.041301713325083256\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 244. loss: 0.04120913986116648\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 245. loss: 0.04111719876527786\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 246. loss: 0.041025884449481964\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 247. loss: 0.04093519877642393\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 248. loss: 0.04084512125700712\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 249. loss: 0.04075566213577986\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 250. loss: 0.04066680837422609\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 251. loss: 0.040578546933829784\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 252. loss: 0.04049088107421994\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 253. loss: 0.04040379682555795\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 254. loss: 0.040317298378795385\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 255. loss: 0.040231368970125914\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 256. loss: 0.04014601465314627\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 257. loss: 0.04006121912971139\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 258. loss: 0.03997697960585356\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 259. loss: 0.03989329468458891\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 260. loss: 0.03981016157194972\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 261. loss: 0.039727562572807074\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 262. loss: 0.03964550280943513\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 263. loss: 0.03956397669389844\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 264. loss: 0.03948297118768096\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 265. loss: 0.03940248768776655\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 266. loss: 0.03932252014055848\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 267. loss: 0.03924306482076645\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 268. loss: 0.039164114743471146\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 269. loss: 0.03908566711470485\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 270. loss: 0.03900771075859666\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 271. loss: 0.038930254988372326\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 272. loss: 0.03885327745229006\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 273. loss: 0.03877679258584976\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 274. loss: 0.038700776640325785\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 275. loss: 0.038625241722911596\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 276. loss: 0.03855016781017184\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 277. loss: 0.03847556421533227\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 278. loss: 0.038401417434215546\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 279. loss: 0.038327730260789394\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 280. loss: 0.03825449291616678\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 281. loss: 0.038181701209396124\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 282. loss: 0.038109354209154844\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 283. loss: 0.03803745051845908\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 284. loss: 0.03796598454937339\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 285. loss: 0.03789494326338172\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 286. loss: 0.037824337370693684\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 287. loss: 0.03775414824485779\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 288. loss: 0.037684382405132055\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 289. loss: 0.037615031469613314\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 290. loss: 0.037546091713011265\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 291. loss: 0.037477567326277494\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 292. loss: 0.03740944294258952\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 293. loss: 0.03734171995893121\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 294. loss: 0.037274394650012255\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 295. loss: 0.03720746608451009\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 296. loss: 0.037140924483537674\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 297. loss: 0.0370747777633369\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 298. loss: 0.037009008694440126\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 299. loss: 0.03694362426176667\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 300. loss: 0.036878619343042374\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 301. loss: 0.03681398509070277\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 302. loss: 0.036749720107764006\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 303. loss: 0.03668582625687122\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 304. loss: 0.03662229282781482\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 305. loss: 0.03655912587419152\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 306. loss: 0.036496319342404604\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 307. loss: 0.03643385739997029\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 308. loss: 0.036371753085404634\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 309. loss: 0.03630999755114317\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 310. loss: 0.03624859033152461\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 311. loss: 0.036187526769936085\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 312. loss: 0.03612680407240987\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 313. loss: 0.036066416185349226\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 314. loss: 0.03600636404007673\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 315. loss: 0.03594664391130209\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 316. loss: 0.0358872557990253\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 317. loss: 0.035828191321343184\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 318. loss: 0.03576944628730416\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 319. loss: 0.03571102814748883\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 320. loss: 0.035652931313961744\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 321. loss: 0.035595146007835865\n",
      "Test_acc 0.9333333333333333\n",
      "-----------------------\n",
      "Epoch 322. loss: 0.03553767455741763\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 323. loss: 0.03548051556572318\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 324. loss: 0.03542366158217192\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 325. loss: 0.03536711446940899\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 326. loss: 0.03531087329611182\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 327. loss: 0.03525493061169982\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 328. loss: 0.035199282225221395\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 329. loss: 0.035143937449902296\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 330. loss: 0.03508888324722648\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 331. loss: 0.03503411682322621\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 332. loss: 0.03497964469715953\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 333. loss: 0.034925447311252356\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 334. loss: 0.03487154236063361\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 335. loss: 0.03481792099773884\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 336. loss: 0.0347645771689713\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 337. loss: 0.03471150668337941\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 338. loss: 0.03465871186926961\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 339. loss: 0.03460620017722249\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 340. loss: 0.03455395018681884\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 341. loss: 0.034501970279961824\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 342. loss: 0.03445025999099016\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 343. loss: 0.03439881233498454\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 344. loss: 0.03434763289988041\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 345. loss: 0.03429670026525855\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 346. loss: 0.03424603492021561\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 347. loss: 0.034195632208138704\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 348. loss: 0.0341454716399312\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 349. loss: 0.034095574636012316\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 350. loss: 0.03404591837897897\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 351. loss: 0.0339965233579278\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 352. loss: 0.03394736535847187\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 353. loss: 0.03389845369383693\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 354. loss: 0.03384979208931327\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 355. loss: 0.03380136098712683\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 356. loss: 0.033753173891454935\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 357. loss: 0.03370522800832987\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 358. loss: 0.033657514955848455\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 359. loss: 0.033610035199671984\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 360. loss: 0.03356278873980045\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 361. loss: 0.03351577464491129\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 362. loss: 0.03346898639574647\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 363. loss: 0.03342242864891887\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 364. loss: 0.03337609674781561\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 365. loss: 0.03332999162375927\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 366. loss: 0.03328410116955638\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 367. loss: 0.033238432835787535\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 368. loss: 0.0331929842941463\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 369. loss: 0.033147755078971386\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 370. loss: 0.033102741464972496\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 371. loss: 0.03305794158950448\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 372. loss: 0.033013352658599615\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 373. loss: 0.03296897932887077\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 374. loss: 0.03292480995878577\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 375. loss: 0.03288085479289293\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 376. loss: 0.03283710265532136\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 377. loss: 0.03279355261474848\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 378. loss: 0.032750210259109735\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 379. loss: 0.03270707232877612\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 380. loss: 0.03266413090750575\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 381. loss: 0.03262138972058892\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 382. loss: 0.03257884597405791\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 383. loss: 0.03253649827092886\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 384. loss: 0.032494349870830774\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 385. loss: 0.03245239099487662\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 386. loss: 0.03241062769666314\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 387. loss: 0.03236905299127102\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 388. loss: 0.032327668741345406\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 389. loss: 0.03228646935895085\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 390. loss: 0.03224546695128083\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 391. loss: 0.03220464335754514\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 392. loss: 0.03216400649398565\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 393. loss: 0.032123550307005644\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 394. loss: 0.03208327805623412\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 395. loss: 0.0320431855507195\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 396. loss: 0.03200327232480049\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 397. loss: 0.03196353977546096\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 398. loss: 0.03192398324608803\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 399. loss: 0.03188460413366556\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 400. loss: 0.03184539917856455\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 401. loss: 0.031806365586817265\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 402. loss: 0.03176750708371401\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 403. loss: 0.03172881156206131\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 404. loss: 0.031690297182649374\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 405. loss: 0.03165194857865572\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 406. loss: 0.031613764353096485\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 407. loss: 0.031575745437294245\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 408. loss: 0.03153789881616831\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 409. loss: 0.03150021191686392\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 410. loss: 0.03146269079297781\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 411. loss: 0.03142532892525196\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 412. loss: 0.031388132367283106\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 413. loss: 0.03135108854621649\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 414. loss: 0.03131420910358429\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 415. loss: 0.03127748565748334\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 416. loss: 0.031240920536220074\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 417. loss: 0.031204513274133205\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 418. loss: 0.03116825968027115\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 419. loss: 0.03113216022029519\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 420. loss: 0.031096212100237608\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 421. loss: 0.031060414388775826\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 422. loss: 0.031024770811200142\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 423. loss: 0.030989276245236397\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 424. loss: 0.030953932087868452\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 425. loss: 0.03091873647645116\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 426. loss: 0.030883686617016792\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 427. loss: 0.03084877971559763\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 428. loss: 0.03081401949748397\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 429. loss: 0.03077940410003066\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 430. loss: 0.03074492933228612\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 431. loss: 0.030710598919540644\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 432. loss: 0.030676412861794233\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 433. loss: 0.03064236743375659\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 434. loss: 0.03060845658183098\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 435. loss: 0.03057469194754958\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 436. loss: 0.03054105956107378\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 437. loss: 0.030507573392242193\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 438. loss: 0.03047420969232917\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 439. loss: 0.030440987553447485\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 440. loss: 0.03040790418162942\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 441. loss: 0.030374948401004076\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 442. loss: 0.030342129059135914\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 443. loss: 0.030309441033750772\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 444. loss: 0.030276877339929342\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 445. loss: 0.030244453344494104\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 446. loss: 0.030212160665541887\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 447. loss: 0.030179991852492094\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 448. loss: 0.030147952027618885\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 449. loss: 0.030116036534309387\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 450. loss: 0.030084250960499048\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 451. loss: 0.03005258971825242\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 452. loss: 0.030021057929843664\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 453. loss: 0.02998964674770832\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 454. loss: 0.029958354774862528\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 455. loss: 0.029927197378128767\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 456. loss: 0.0298961466178298\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 457. loss: 0.029865229967981577\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 458. loss: 0.029834432527422905\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 459. loss: 0.029803745448589325\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 460. loss: 0.029773185029625893\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 461. loss: 0.029742740094661713\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 462. loss: 0.029712412506341934\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 463. loss: 0.029682203195989132\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 464. loss: 0.02965210797265172\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 465. loss: 0.029622134752571583\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 466. loss: 0.029592270497232676\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 467. loss: 0.029562523122876883\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 468. loss: 0.029532890301197767\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 469. loss: 0.02950336877256632\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 470. loss: 0.029473962262272835\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 471. loss: 0.029444664251059294\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 472. loss: 0.029415479861199856\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 473. loss: 0.029386403504759073\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 474. loss: 0.029357430525124073\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 475. loss: 0.029328578617423773\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 476. loss: 0.02929982589557767\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 477. loss: 0.029271184466779232\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 478. loss: 0.029242653399705887\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 479. loss: 0.029214221984148026\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 480. loss: 0.02918589673936367\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 481. loss: 0.029157685581594706\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 482. loss: 0.029129568953067064\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 483. loss: 0.02910156175494194\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 484. loss: 0.029073658864945173\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 485. loss: 0.02904585236683488\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 486. loss: 0.02901815762743354\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 487. loss: 0.028990556485950947\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 488. loss: 0.02896306151524186\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 489. loss: 0.02893566759303212\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 490. loss: 0.02890836726874113\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 491. loss: 0.028881167992949486\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 492. loss: 0.02885407255962491\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 493. loss: 0.028827072586864233\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 494. loss: 0.02880017226561904\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 495. loss: 0.028773367404937744\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 496. loss: 0.02874666266143322\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 497. loss: 0.028720047790557146\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 498. loss: 0.02869353536516428\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n",
      "Epoch 499. loss: 0.02866711327806115\n",
      "Test_acc 0.9666666666666667\n",
      "-----------------------\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 导入包\n",
    "import numpy as mp\n",
    "import tensorflow as tf\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# 划分数据集\n",
    "x_data = load_iris().data\n",
    "y_data = load_iris().target\n",
    "# 随机分组\n",
    "np.random.seed(111)\n",
    "np.random.shuffle(x_data)\n",
    "np.random.seed(111)\n",
    "np.random.shuffle(y_data)\n",
    "\n",
    "x_train, y_train = x_data[:-30], y_data[:-30]\n",
    "x_test, y_test = x_data[-30:], y_data[-30:]\n",
    "x_train, x_test = tf.cast(x_train, tf.float32), tf.cast(x_test, tf.float32)\n",
    "train_db = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32)\n",
    "test_db = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(32)\n",
    "\n",
    "# 搭建网络\n",
    "w = tf.Variable(tf.random.truncated_normal((4, 3), stddev=0.1, seed=1))\n",
    "b = tf.Variable(tf.random.truncated_normal((3,), stddev=0.1,seed=1))\n",
    "\n",
    "# 设置超参数\n",
    "lr = 0.1\n",
    "train_loss_results = []\n",
    "test_acc = []\n",
    "epochs = 500\n",
    "loss_all = 0\n",
    "\n",
    "# 训练模型\n",
    "for epoch in range(epochs):\n",
    "    for step, (x_train, y_train) in enumerate(train_db):\n",
    "        with tf.GradientTape() as tape:\n",
    "            y = tf.matmul(x_train, w) + b\n",
    "            y = tf.nn.softmax(y)\n",
    "            y_ = tf.one_hot(y_train, depth=3)\n",
    "            loss = tf.reduce_mean(tf.square(y_ - y))\n",
    "            loss_all += loss.numpy()\n",
    "        grads = tape.gradient(loss, [w, b])\n",
    "\n",
    "        w.assign_sub(lr * grads[0])\n",
    "        b.assign_sub(lr * grads[1])\n",
    "\n",
    "    print(\"Epoch {}. loss: {}\".format(epoch, loss_all/4))\n",
    "    train_loss_results.append(loss_all / 4)\n",
    "    loss_all = 0\n",
    "\n",
    "    total_correct, total_number = 0, 0\n",
    "    for x_test, y_test in test_db:\n",
    "        y = tf.matmul(x_test, w) + b\n",
    "        y = tf.nn.softmax(y)\n",
    "        pred = tf.argmax(y, axis=1)\n",
    "        pred = tf.cast(pred, dtype=y_test.dtype)\n",
    "        correct = tf.cast(tf.equal(pred, y_test), dtype=tf.int32)\n",
    "        correct = tf.reduce_sum(correct)\n",
    "        total_correct += int(correct)\n",
    "        total_number += x_test.shape[0]\n",
    "    acc = total_correct / total_number\n",
    "    test_acc.append(acc)\n",
    "    print(\"Test_acc\", acc)\n",
    "    print(\"-----------------------\")\n",
    "\n",
    "# 绘图\n",
    "plt.title(\"Loss Function Curve\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.plot(train_loss_results, label=\"$Losss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 绘制 Accuracy 曲线\n",
    "plt.title(\"Acc Curve\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Acc\")\n",
    "plt.plot(test_acc, label=\"$Accuracy$\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss 0.22117557376623154\n",
      "Test_acc:  0.23333333333333334\n",
      "------------------------\n",
      "Epoch 100, loss 0.06599666550755501\n",
      "Test_acc:  0.9666666666666667\n",
      "------------------------\n",
      "Epoch 200, loss 0.045768097043037415\n",
      "Test_acc:  0.9666666666666667\n",
      "------------------------\n",
      "Epoch 300, loss 0.03667847579345107\n",
      "Test_acc:  0.9666666666666667\n",
      "------------------------\n",
      "Epoch 400, loss 0.03158428240567446\n",
      "Test_acc:  0.9666666666666667\n",
      "------------------------\n",
      "Epoch 500, loss 0.02831731247715652\n",
      "Test_acc:  0.9666666666666667\n",
      "------------------------\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 背下来！\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "import tensorflow as tf\n",
    "\n",
    "# 加载数据随机打乱\n",
    "x_data = load_iris().data\n",
    "y_data = load_iris().target\n",
    "np.random.seed(120)\n",
    "np.random.shuffle(x_data)\n",
    "np.random.seed(120)\n",
    "np.random.shuffle(y_data)\n",
    "\n",
    "# 划分数据集\n",
    "x_train = x_data[:-30]\n",
    "y_train = y_data[:-30]\n",
    "x_test = x_data[-30:]\n",
    "y_test = y_data[-30:]\n",
    "\n",
    "# 数据类型转换\n",
    "x_train, x_test = tf.cast(x_train, tf.float32), tf.cast(x_test, tf.float32)\n",
    "\n",
    "# batch\n",
    "train_db = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32)\n",
    "test_db = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(32)\n",
    "\n",
    "# 可训练参数\n",
    "w = tf.Variable(tf.random.truncated_normal((4, 3), mean=0, stddev=0.1, seed=1))\n",
    "b = tf.Variable(tf.random.truncated_normal((3,), mean=0, stddev=0.1, seed=1))\n",
    "\n",
    "# 超参数\n",
    "lr = 0.1\n",
    "epochs = 500\n",
    "train_loss_results = []\n",
    "test_acc = []\n",
    "\n",
    "# 对所有数据的epoch循环\n",
    "for epoch in range(epochs+1):\n",
    "    # 训练模型\n",
    "    loss_all = 0\n",
    "    for step, (x_train, y_train) in enumerate(train_db):\n",
    "        with tf.GradientTape() as tape:\n",
    "            y = tf.matmul(x_train, w) + b\n",
    "            y = tf.nn.softmax(y)    # 输出符合概率分布\n",
    "            y_ = tf.one_hot(y_train, depth=3)\n",
    "            loss = tf.reduce_mean(tf.square(y - y_))\n",
    "            loss_all += loss.numpy()\n",
    "        grads = tape.gradient(loss, [w, b])\n",
    "\n",
    "        w.assign_sub(lr * grads[0])\n",
    "        b.assign_sub(lr * grads[1])\n",
    "        \n",
    "    if epoch % 100 == 0:\n",
    "        print(\"Epoch {}, loss {}\".format(epoch, loss_all/4))\n",
    "    train_loss_results.append(loss_all / 4)\n",
    "\n",
    "    # 测试\n",
    "    total_correct, total_number = 0, 0\n",
    "    for x_test, y_test in test_db:\n",
    "        y = tf.matmul(x_test, w) + b\n",
    "        y = tf.nn.softmax(y)\n",
    "        pred = tf.argmax(y, axis=1)\n",
    "        pred = tf.cast(pred, dtype=y_test.dtype)\n",
    "        correct = tf.cast(tf.equal(pred, y_test), dtype=tf.int32)\n",
    "        total_correct += int(tf.reduce_sum(correct))\n",
    "        total_number += x_test.shape[0]\n",
    "    acc = total_correct / total_number\n",
    "    test_acc.append(acc)\n",
    "    if epoch % 100 == 0:\n",
    "        print(\"Test_acc: \", acc)\n",
    "        print(\"------------------------\")\n",
    "\n",
    "# 绘图\n",
    "plt.title(\"Loss Function Curve\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.plot(train_loss_results, label=\"$Loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 绘制 Accuracy 曲线\n",
    "plt.title(\"Acc Curve\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Acc\")\n",
    "plt.plot(test_acc, label=\"$Accuracy$\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.0 ('tensorflow2.4')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4187bdf84c6397f9f6c7d9023f551e0c8ae8694ff2647aa1a9db48255188d261"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
